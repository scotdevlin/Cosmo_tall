{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c731c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's set up our packages\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import integrate\n",
    "\n",
    "# And set some constants\n",
    "c = 299792.458 # km/s (speed of light)\n",
    "H0kmsmpc = 70.  # Hubble constant in km/s/Mpc\n",
    "H0s = H0kmsmpc * 3.2408e-20 # H0 in inverse seconds is H0 in km/s/Mpc * (3.2408e-20 Mpc/km)\n",
    "\n",
    "# Write a function for the integrand, i.e. $1/E(z)$,\n",
    "def ezinv(z,om=0.3,ol=0.7,w0=-1.0,wa=0.0,orr=0.0):\n",
    "    ok = 1.-om-ol-orr\n",
    "    a  = 1./(1.+z)\n",
    "    f = a**(3*(1.+w0+wa)) * np.exp(3*wa*(1-a))\n",
    "    ez = np.sqrt( orr/a**4 + om/a**3 + ok/a**2 + ol/f )\n",
    "    return 1./ez\n",
    "\n",
    "# The curvature correction function\n",
    "def Sk(xx, ok):\n",
    "    if ok < 0.0:\n",
    "        dk = np.sin(np.sqrt(-ok)*xx)/np.sqrt(-ok)\n",
    "    elif ok > 0.0:\n",
    "        dk = np.sinh(np.sqrt(ok)*xx)/np.sqrt(ok)\n",
    "    else:\n",
    "        dk = xx\n",
    "    return dk\n",
    "\n",
    "# The distance modulus\n",
    "def dist_mod(zs,om=0.3,ol=0.7,w0=-1.0,wa=0.0,orr=0.0):\n",
    "    \"\"\" Calculate the distance modulus, correcting for curvature\"\"\"\n",
    "    ok = 1.0 - om - ol\n",
    "    z_array = np.linspace(0.01,1,50)\n",
    "    xx_array = np.array([integrate.quad(ezinv, 0, z, args=(om, ol, w0, wa, orr))[0] for z in z_array])\n",
    "    xx = np.interp(zs, z_array, xx_array)\n",
    "    #xx = np.array([integrate.quad(ezinv, 0, z, args=(om, ol, w0, wa, orr))[0] for z in zs])\n",
    "    D = Sk(xx, ok)\n",
    "    lum_dist = D * (1 + zs) \n",
    "    dist_mod = 5 * np.log10(lum_dist) # Distance modulus\n",
    "    # Add an arbitrary constant that's approximately the log of c on Hubble constant minus absolute magnitude of -19.5\n",
    "    dist_mod = dist_mod + np.log(c/H0kmsmpc)-(-19.5)  # You can actually skip this step and it won't make a difference to our fitting\n",
    "    return dist_mod\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d75ccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new function that reads in the data (data files should be in a directory called data)\n",
    "def read_data(model_name):\n",
    "    d = np.genfromtxt('data/'+model_name+'.txt',delimiter=',')\n",
    "    zs = d[:,0]\n",
    "    mu = d[:,1]\n",
    "    muerr=d[:,2]\n",
    "    # Sort the arrays based on zs\n",
    "    sort_idx = np.argsort(zs)\n",
    "    zs = zs[sort_idx]\n",
    "    mu = mu[sort_idx]\n",
    "    muerr = muerr[sort_idx]\n",
    "    return zs, mu, muerr\n",
    "\n",
    "zs, mu, muerr = read_data('realdata')\n",
    "\n",
    "#################\n",
    "# Temporarily halve muerr for testing\n",
    "#muerr = muerr/2 \n",
    "#################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b06e3548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the arrays for the models you want to test, e.g. a range of Omega_m and Omega_Lambda models:\n",
    "n = 21                   # Increase this for a finer grid\n",
    "oms = np.linspace(0.1, 0.5, n)   # Array of matter densities\n",
    "ols = np.linspace(0.4, 1.0, n)   # Array of cosmological constant values\n",
    "w0s = np.linspace(-1.5, -0.5, n)   # Array of cosmological constant values\n",
    "wa = 0\n",
    "\n",
    "chi2_3d = np.ones((n, n, n)) * np.inf  # Array to hold our chi2 values, set initially to super large values   \n",
    "chi2_3d_prior = np.ones((n, n, n)) * np.inf  # Array to hold our chi2 values, set initially to super large values  \n",
    "chi2_3d_prior2 = np.ones((n, n, n)) * np.inf\n",
    "chi2_3d_withprior = np.ones((n, n, n)) * np.inf  # Array to hold our chi2 values, set initially to super large values   \n",
    "\n",
    "#LOAD_SAVED=True #for saving runs\n",
    "#if LOAD_SAVED:\n",
    "#   chi2_3d = np.load('chi2_3d.npy')  \n",
    "#   print(chi2_3d)   \n",
    "#else:    \n",
    "    # Calculate Chi2 for each model\n",
    "for i, om in enumerate(oms):                                          # loop through matter densities\n",
    "    chi2_3d_prior2[i,:,:] = ((om-0.27)/0.03)**2                 #prior to matter\n",
    "    for j, ol in enumerate(ols):                                  # loop through cosmological constant densities\n",
    "        chi2_3d_prior[i,j,:] = ((om+ol-1.0)/0.02)**2              #flatness prior/result from handout\n",
    "        for k, w0 in enumerate(w0s):\n",
    "            mu_model = dist_mod(zs, om=om, ol=ol, w0=w0)              # calculate the distance modulus vs redshift for that model \n",
    "            mu_model_norm = mu_model-np.sum((mu_model-mu)/muerr**2)/np.sum(1./muerr**2) # Apply the vertical offset\n",
    "            chi2_3d[i,j,k] = np.sum((mu_model_norm - mu) ** 2 / muerr**2)  # Calculate the chi2 and save it in a matrix \n",
    "chi2_3d_withprior = chi2_3d+chi2_3d_prior+chi2_3d_prior2 #total prior\n",
    "#np.save('chi2_3d.npy',chi2_3d)\n",
    "#np.save('chi2_3d_withprior.npy',chi2_3d_withprior)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d8019a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.2\n",
      "Best fit values are (om,ol,w0)=(0.160,0.400,-1.200)\n",
      "Reduced chi^2 for the best fit is 1.70\n",
      "-0.8999999999999999\n",
      "Best fit values are (om,ol,w0)=(0.260,0.730,-0.900)\n",
      "Reduced chi^2 for the best fit is 1.70\n"
     ]
    }
   ],
   "source": [
    "##For no prior\n",
    "# Convert that to a likelihood and calculate the reduced chi2\n",
    "likelihood_3d = np.exp(-0.5 * (chi2_3d-np.amin(chi2_3d)))  # convert the chi^2 to a likelihood (np.amin(chi2) calculates the minimum of the chi^2 array)\n",
    "likelihood_3d = likelihood_3d /np.sum(likelihood_3d)        # normalise so the total likelihood = 1.0.\n",
    "chi2_reduced = chi2_3d / (len(mu)-2)                       # calculate the reduced chi^2, i.e. chi^2 per degree of freedom, where dof = number of data points minus number of parameters being fitted \n",
    "\n",
    "# Calculate the best fit values (where chi2 is minimum)\n",
    "indbest_3d = np.argmin(chi2_3d)                 # Gives index of best fit but where the indices are just a single number\n",
    "ibest_3d   = np.unravel_index(indbest_3d,[n,n,n]) # Converts the best fit index to the 3d version (i,j,k)\n",
    "print(w0s[ibest_3d[2]])\n",
    "print( 'Best fit values are (om,ol,w0)=(%.3f,%.3f,%.3f)'%( oms[ibest_3d[0]], ols[ibest_3d[1]], w0s[ibest_3d[2]] ) )\n",
    "print( 'Reduced chi^2 for the best fit is %0.2f'%chi2_reduced[ibest_3d[0],ibest_3d[1],ibest_3d[2]] )\n",
    "\n",
    "# Turn 3d likelihoods into 2d arrays \n",
    "likelihood_3domol = np.sum(likelihood_3d,2)\n",
    "likelihood_3domw0 = np.sum(likelihood_3d,1)\n",
    "likelihood_3dolw0 = np.sum(likelihood_3d,0)\n",
    "\n",
    "# Turn 2d likelihoods into 1d arrays\n",
    "likelihood_3dom = np.sum(likelihood_3domol,1)\n",
    "likelihood_3dol = np.sum(likelihood_3dolw0,1)\n",
    "likelihood_3dw0 = np.sum(likelihood_3dolw0,0)\n",
    "\n",
    "\n",
    "##FOR flat prior\n",
    "# Convert that to a likelihood and calculate the reduced chi2\n",
    "likelihood_3d_flat = np.exp(-0.5 * (chi2_3d_prior-np.amin(chi2_3d_prior)))  # convert the chi^2 to a likelihood (np.amin(chi2) calculates the minimum of the chi^2 array)\n",
    "likelihood_3d_flat = likelihood_3d_flat /np.sum(likelihood_3d_flat)        # normalise so the total likelihood = 1.0.\n",
    "\n",
    "# Turn 3d likelihoods into 2d arrays \n",
    "likelihood_3domol_flat = np.sum(likelihood_3d_flat,2)\n",
    "likelihood_3domw0_flat = np.sum(likelihood_3d_flat,1)\n",
    "likelihood_3dolw0_flat = np.sum(likelihood_3d_flat,0)\n",
    "\n",
    "\n",
    "##FOR matter prior\n",
    "# Convert that to a likelihood and calculate the reduced chi2\n",
    "likelihood_3d_mat = np.exp(-0.5 * (chi2_3d_prior2-np.amin(chi2_3d_prior2)))  # convert the chi^2 to a likelihood (np.amin(chi2) calculates the minimum of the chi^2 array)\n",
    "likelihood_3d_mat = likelihood_3d_mat /np.sum(likelihood_3d_mat)        # normalise so the total likelihood = 1.0.\n",
    "\n",
    "# Turn 3d likelihoods into 2d arrays \n",
    "likelihood_3domol_mat = np.sum(likelihood_3d_mat,2)\n",
    "likelihood_3domw0_mat = np.sum(likelihood_3d_mat,1)\n",
    "likelihood_3dolw0_mat = np.sum(likelihood_3d_mat,0)\n",
    "\n",
    "\n",
    "\n",
    "##FOR PRIORS\n",
    "likelihood_3d_prior = np.exp(-0.5 * (chi2_3d_withprior-np.amin(chi2_3d)))  # convert the chi^2 to a likelihood (np.amin(chi2) calculates the minimum of the chi^2 array)\n",
    "likelihood_3d_prior = likelihood_3d_prior /np.sum(likelihood_3d_prior)        # normalise so the total likelihood = 1.0.\n",
    "chi2_reduced_prior = chi2_3d_withprior / (len(mu)-2)                       # calculate the reduced chi^2, i.e. chi^2 per degree of freedom, where dof = number of data points minus number of parameters being fitted \n",
    "\n",
    "# Calculate the best fit values (where chi2 is minimum)\n",
    "indbest_3d_prior = np.argmin(chi2_3d_withprior)                 # Gives index of best fit but where the indices are just a single number\n",
    "ibest_3d_prior   = np.unravel_index(indbest_3d_prior,[n,n,n]) # Converts the best fit index to the 3d version (i,j,k)\n",
    "print(w0s[ibest_3d_prior[2]])\n",
    "print( 'Best fit values are (om,ol,w0)=(%.3f,%.3f,%.3f)'%( oms[ibest_3d_prior[0]], ols[ibest_3d_prior[1]], w0s[ibest_3d_prior[2]] ) )\n",
    "print( 'Reduced chi^2 for the best fit is %0.2f'%chi2_reduced[ibest_3d_prior[0],ibest_3d_prior[1],ibest_3d_prior[2]] )\n",
    "\n",
    "# Turn 3d likelihoods into 2d arrays \n",
    "likelihood_3domol_prior = np.sum(likelihood_3d_prior,2)\n",
    "likelihood_3domw0_prior = np.sum(likelihood_3d_prior,1)\n",
    "likelihood_3dolw0_prior = np.sum(likelihood_3d_prior,0)\n",
    "\n",
    "# Turn 2d likelihoods into 1d arrays\n",
    "likelihood_3dom_prior = np.sum(likelihood_3domol_prior,1)\n",
    "likelihood_3dol_prior = np.sum(likelihood_3dolw0_prior,1)\n",
    "likelihood_3dw0_prior = np.sum(likelihood_3dolw0_prior,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7df15141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best fit values in 2D are ($\\Omega_m$,$\\Omega_l$)=(0.260,0.730)\n",
      "Best fit values in 2D are ($\\Omega_m$,$\\Omega_l$)=(0.260,0.730)\n",
      "Best fit values in 2D are ($\\Omega_m$,$w_0$)=(0.260,-0.900)\n",
      "Best fit values in 2D are ($\\Omega_m$,$w_0$)=(0.260,-0.900)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nvhin\\AppData\\Local\\Temp\\ipykernel_17412\\1329012844.py:30: UserWarning: No contour levels were found within the data range.\n",
      "  plt.contour(x,y,np.transpose(pri1-np.amin(pri1)),cmap=\"winter\",**{'levels':levels}) #flatness prior\n",
      "C:\\Users\\nvhin\\AppData\\Local\\Temp\\ipykernel_17412\\1329012844.py:31: UserWarning: No contour levels were found within the data range.\n",
      "  plt.contour(x,y,np.transpose(pri2-np.amin(pri2)),cmap=\"winter\",**{'levels':levels}) #matter prior\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best fit values in 2D are ($\\Omega_l$,$w_0$)=(0.730,-0.900)\n",
      "Best fit values in 2D are ($\\Omega_l$,$w_0$)=(0.730,-0.900)\n"
     ]
    }
   ],
   "source": [
    "def find_levels(likearr2d):\n",
    "\t# find the likelihood level that encloses 1sigma, 2sigma, and 3sigma of the likelihood (approximately)\n",
    "\tnlevel=50\n",
    "\ttot = np.zeros(nlevel)\n",
    "\tlevels = np.linspace(0,np.max(likearr2d),nlevel)\n",
    "\tfor i,level in enumerate(levels):\n",
    "\t\ttot[i] = np.sum(likearr2d[likearr2d>level])\n",
    "\tisig1 = (np.abs(tot - 0.6827)).argmin()\n",
    "\tisig2 = (np.abs(tot - 0.9245)).argmin()\n",
    "\tisig3 = (np.abs(tot - 0.9973)).argmin()\n",
    "\treturn np.array([levels[isig3],levels[isig2],levels[isig1]])\n",
    "\n",
    "\n",
    "def plot_1d(x,y,xlabel,ylabel,xbest,filename='plots/temp.png'):\n",
    "\tplt.plot(x,y)\n",
    "\tplt.axvline(x=xbest)\n",
    "\tplt.xlabel(xlabel, fontsize=12)\n",
    "\tplt.ylabel(ylabel, fontsize=12)\n",
    "\tplt.savefig(filename,bbox_inches='tight')\n",
    "\tplt.close()\n",
    "\n",
    "def plot_contour(x,y,z,pri1,pri2,pri_tot,xlabel,ylabel,xbest,ybest,filename='plots/temp.png'):\n",
    "#def plot_contour(x,y,z,xlabel,ylabel,xbest,ybest,filename='plots/temp.png'):\n",
    "    indbest_2d = np.argmax(pri_tot)             # Gives index of best fit but where the indices are just a single number\n",
    "    ibest_2d   = np.unravel_index(indbest_2d,[n,n]) # Converts the best fit index to the 3d version (i,j,k)\n",
    "    print( 'Best fit values in 2D are (%s,%s)=(%.3f,%.3f)'%( xlabel,ylabel,x[ibest_2d[0]], y[ibest_2d[1]]))\n",
    "    print( 'Best fit values in 2D are (%s,%s)=(%.3f,%.3f)'%( xlabel,ylabel,xbest, ybest))\n",
    "    levels = find_levels(z)\n",
    "    plt.contour(x,y,np.transpose(z-np.amin(z)),cmap=\"winter\",**{'levels':levels}) #basic data\n",
    "    plt.contour(x,y,np.transpose(pri1-np.amin(pri1)),cmap=\"winter\",**{'levels':levels}) #flatness prior\n",
    "    plt.contour(x,y,np.transpose(pri2-np.amin(pri2)),cmap=\"winter\",**{'levels':levels}) #matter prior\n",
    "    plt.contour(x,y,np.transpose(pri_tot-np.amin(pri_tot)),cmap=\"winter\",**{'levels':levels}) #matter prior\n",
    "    plt.plot(x[ibest_2d[0]], y[ibest_2d[1]],'x',color='black',label='(%s,%s)=(%.3f,%.3f)'%(xlabel,ylabel,x[ibest_2d[0]],y[ibest_2d[1]]) )\n",
    "    plt.plot(xbest, ybest,'o',color='black',label='(%s,%s)=(%.3f,%.3f)'%(xlabel,ylabel,xbest,ybest) )\n",
    "    plt.xlabel(xlabel, fontsize=12)\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    plt.legend(frameon=False)\n",
    "    plt.savefig(filename,bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "#For ALL\n",
    "plot_contour(oms,ols,likelihood_3domol,likelihood_3domol_flat,likelihood_3domol_mat,likelihood_3domol_prior,\"$\\Omega_m$\",\"$\\Omega_l$\",oms[ibest_3d_prior[0]],ols[ibest_3d_prior[1]],filename='plots/contours_omol.png',)\n",
    "plot_contour(oms,w0s,likelihood_3domw0,likelihood_3domw0_flat,likelihood_3domw0_mat,likelihood_3domw0_prior,\"$\\Omega_m$\",\"$w_0$\",oms[ibest_3d_prior[0]],w0s[ibest_3d_prior[2]],filename='plots/contours_omw0.png')\n",
    "plot_contour(ols,w0s,likelihood_3dolw0,likelihood_3dolw0_flat,likelihood_3dolw0_mat,likelihood_3dolw0_prior,\"$\\Omega_l$\",\"$w_0$\",ols[ibest_3d_prior[1]],w0s[ibest_3d_prior[2]],filename='plots/contours_olw0.png')\n",
    "\n",
    "\n",
    "##FOR No PRIOR\n",
    "#plot_contour(oms,ols,likelihood_3domol,\"$\\Omega_m$\",\"$\\Omega_l$\",oms[ibest_3d[0]],ols[ibest_3d[1]],filename='plots/contours_omol_noprior.png',)\n",
    "#plot_contour(oms,w0s,likelihood_3domw0,\"$\\Omega_m$\",\"$w_0$\",oms[ibest_3d[0]],w0s[ibest_3d[2]],filename='plots/contours_omw0_noprior.png')\n",
    "#plot_contour(ols,w0s,likelihood_3dolw0,\"$\\Omega_l$\",\"$w_0$\",ols[ibest_3d[1]],w0s[ibest_3d[2]],filename='plots/contours_olw0_noprior.png')\n",
    "\n",
    "\n",
    "##FOR total PRIOR\n",
    "#plot_contour(oms,ols,likelihood_3domol,\"$\\Omega_m$\",\"$\\Omega_l$\",oms[ibest_3d_prior[0]],ols[ibest_3d_prior[1]],filename='plots/contours_omol_prior.png',)\n",
    "#plot_contour(oms,w0s,likelihood_3domw0,\"$\\Omega_m$\",\"$w_0$\",oms[ibest_3d_prior[0]],w0s[ibest_3d_prior[2]],filename='plots/contours_omw0_prior.png')\n",
    "#plot_contour(ols,w0s,likelihood_3dolw0,\"$\\Omega_l$\",\"$w_0$\",ols[ibest_3d_prior[1]],w0s[ibest_3d_prior[2]],filename='plots/contours_olw0_prior.png')\n",
    "\n",
    "##1d likelywood plots for all\n",
    "plot_1d(oms,likelihood_3dom_prior,\"$\\Omega_m$\",\"Likelihood\",oms[ibest_3d_prior[0]],filename='plots/like_om_prior.png')\n",
    "plot_1d(ols,likelihood_3dol_prior,\"$\\Omega_l$\",\"Likelihood\",ols[ibest_3d_prior[1]],filename='plots/like_ol_prior.png')\n",
    "plot_1d(w0s,likelihood_3dw0_prior,\"$w_0$\",\"Likelihood\",w0s[ibest_3d_prior[2]],filename='plots/like_w0_prior.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738d4e7a",
   "metadata": {},
   "source": [
    "Current problems for prior plots:\n",
    "1. There is a ol prior showing up in the olw0 plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a584a5e7",
   "metadata": {},
   "source": [
    "Notes about plots:\n",
    "1. for plotting, all the dots and crosses are the best for the total prior plot\n",
    "2. when comparing the all plot and total prior plot the central contours change size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
