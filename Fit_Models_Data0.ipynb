{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9317a860",
   "metadata": {},
   "source": [
    "# 3. Fitting models to data\n",
    "\n",
    "In this workbook we will import data and find the cosmological model that best fits that data. \n",
    "\n",
    "### 3.1 First set up the packages and functions\n",
    "We need to know the distance modulus as a function of redshift.  Distance modulus is defined as \n",
    "\\begin{equation} \\mu(z) = 5 \\log_{10}(D_L)\\end{equation}\n",
    "where\n",
    "\\begin{equation} D_L = R_0 S_k(\\chi) (1+z) \\end{equation}\n",
    "and $S_k(\\chi)=\\sin(\\chi), \\chi,\\; {\\rm or}\\; \\sinh(\\chi)$ for closed, flat, and open universes respectively, and\n",
    "\\begin{equation} R_0\\chi = \\frac{c}{H_0}\\int_0^z \\frac{dz}{E(z)}\\end{equation}\n",
    "with $E(z)=H(z)/H_0$.\n",
    "\n",
    "Note that the absolute magnitude of the SNe is about -19.5, but it enters as an additive constant in the same way as $c/H_0$ and both have high uncertainty.  Therefore we *marginalise* over this offset and it doesn't matter what value of $H_0$ we use.\n",
    "\n",
    "#### Important: In the code below, replace ez = 1.0 with the actual function for E(z).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d80028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's set up our packages\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import integrate\n",
    "\n",
    "# And set some constants\n",
    "c = 299792.458 # km/s (speed of light)\n",
    "H0kmsmpc = 70.  # Hubble constant in km/s/Mpc\n",
    "H0s = H0kmsmpc * 3.2408e-20 # H0 in inverse seconds is H0 in km/s/Mpc * (3.2408e-20 Mpc/km)\n",
    "\n",
    "# Write a function for the integrand, i.e. $1/E(z)$,\n",
    "def ezinv(z,om=0.3,ol=0.7,w0=-1.0,wa=0.0,orr=0.0):\n",
    "    ok = 1.-om-ol-orr\n",
    "    ez = np.sqrt((om*(1+z)**3)+(ok*(1+z)**2)+ol)\n",
    "    return 1./ez\n",
    "\n",
    "# The curvature correction function\n",
    "def Sk(xx, ok):\n",
    "    if ok < 0.0:\n",
    "        dk = np.sin(np.sqrt(-ok)*xx)/np.sqrt(-ok)\n",
    "    elif ok > 0.0:\n",
    "        dk = np.sinh(np.sqrt(ok)*xx)/np.sqrt(ok)\n",
    "    else:\n",
    "        dk = xx\n",
    "    return dk\n",
    "\n",
    "# The distance modulus\n",
    "def dist_mod(zs,om=0.3,ol=0.7,w0=-1.0,wa=0.0,orr=0.0):\n",
    "    \"\"\" Calculate the distance modulus, correcting for curvature\"\"\"\n",
    "    ok = 1.0 - om - ol\n",
    "    xx = np.array([integrate.quad(ezinv, 0, z, args=(om, ol, w0, wa, orr))[0] for z in zs])\n",
    "    D = Sk(xx, ok)\n",
    "    lum_dist = D * (1 + zs) \n",
    "    dist_mod = 5 * np.log10(lum_dist) # Distance modulus\n",
    "    # Add an arbitrary constant that's approximately the log of c on Hubble constant minus absolute magnitude of -19.5\n",
    "    dist_mod = dist_mod + np.log(c/H0kmsmpc)-(-19.5)  # You can actually skip this step and it won't make a difference to our fitting\n",
    "    return dist_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1619f9d",
   "metadata": {},
   "source": [
    "### 3.2 Now read in the data\n",
    "I have generated some mock supernova data at random redshifts between $0.02<z<1.0$.  I then choose a cosmological model and calculate the distance modulus for each redshift in that model.  Then I give some uncertainty to the data points and scatter them randomly about the correct model by an amount corresponding to their uncertainty. \n",
    "\n",
    "I've generated 5 different models, all in files called DataX.txt, where X is a number from 0-4. \n",
    "\n",
    "There are five possible variables that I have specified in the models.  These are\n",
    "* Matter density: $\\Omega_M$\n",
    "* Dark Energy density: $\\Omega_\\Lambda$\n",
    "* Radiation density: $\\Omega_R$\n",
    "* Equation of state of dark energy: $w_0$\n",
    "* Change in the equation of state with scalefactor: $w_a$, where $w=w_0+w_a(1-a)$.\n",
    "\n",
    "You can test your code on Data00.txt and Data0.txt.  Both were generated using a model with $(\\Omega_M,\\Omega_{\\Lambda})=(0.3,0.7)$ and all other parameters set to their standard values, namely $\\Omega_R=0.0$, $w_0=-1.0$, $w_a=0.0$.  In addition Data00.txt has perfect data with no scatter and small uncertainties, so you should be able to recover exactly the correct model. \n",
    "\n",
    "Data sets 1-2 are models in which $\\Omega_R=0.0$, $w_0=-1.0$, $w_a=0.0$ and only $\\Omega_M$ and $\\Omega_{\\Lambda}$ change.  \n",
    "\n",
    "Data sets 3-4 could be anything! (Within the realms of the five parameters described above.)\n",
    "\n",
    "Feel free to choose which data set you want to analyse for your project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e4507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new function that reads in the data (data files should be in a directory called data)\n",
    "def read_data(model_name):\n",
    "    d = np.genfromtxt('data/'+model_name+'.txt',delimiter=',')\n",
    "    zs = d[:,0]\n",
    "    mu = d[:,1]\n",
    "    muerr=d[:,2]\n",
    "    return zs, mu, muerr\n",
    "\n",
    "zs, mu, muerr = read_data('Data1')\n",
    "\n",
    "# Plot it to see what it looks like, this is called a Hubble diagram\n",
    "plt.errorbar(zs,mu,yerr=muerr,fmt='.',elinewidth=0.7,markersize=4,alpha=0.5)\n",
    "plt.xlim(0,1.0)\n",
    "plt.xlabel('redshift')\n",
    "plt.ylabel('magnitude')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8aa08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4ca8d8",
   "metadata": {},
   "source": [
    "### 3.3 Calculate the theory corresponding to that data for a range of cosmological models\n",
    "Then overplot them on the Hubble diagram.  Here we're just testing some specific interesting examples.\n",
    "It's always important to visualise your data and check that the models and data are doing what you think they are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c74bef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_om00_ox00 = dist_mod(zs,om=0.0,ol=0.0)  # We're going to use this empty model as a benchmark to compare the others to\n",
    "mu_om10_ox00 = dist_mod(zs,om=1.0,ol=0.0)\n",
    "mu_om03_ox00 = dist_mod(zs,om=0.3,ol=0.0)\n",
    "mu_om03_ox07 = dist_mod(zs,om=0.3,ol=0.7)\n",
    "\n",
    "# Plot it to see what it looks like, this is called a Hubble diagram\n",
    "plt.errorbar(zs,mu,yerr=muerr,fmt='.',elinewidth=0.7,markersize=4,alpha=0.5)\n",
    "plt.plot(zs,mu_om00_ox00,':',color='black',label='(0.0, 0.0)')\n",
    "plt.plot(zs,mu_om10_ox00,'-.',color='red',label='(1.0, 0.0)')\n",
    "plt.plot(zs,mu_om03_ox00,'--',color='blue',label='(0.3, 0.0)')\n",
    "plt.plot(zs,mu_om03_ox07,'-',color='green',label='(0.3, 0.7)')\n",
    "plt.xlim(0,1.0)\n",
    "plt.xlabel('redshift')\n",
    "plt.ylabel('magnitude')\n",
    "plt.legend(frameon=False)\n",
    "plt.show()\n",
    "\n",
    "# Now plot a Hubble diagram relative to the empty model (i.e. subtract the empty model from all the data and models)\n",
    "plt.errorbar(zs,mu-mu_om00_ox00,yerr=muerr,fmt='.',elinewidth=0.7,markersize=4,alpha=0.5)\n",
    "plt.plot(zs,mu_om10_ox00-mu_om00_ox00,'-.',color='red',label='(1.0, 0.0)')\n",
    "plt.plot(zs,mu_om03_ox00-mu_om00_ox00,'--',color='blue',label='(0.3, 0.0)')\n",
    "plt.plot(zs,mu_om03_ox07-mu_om00_ox00,'-',color='green',label='(0.3, 0.7)')\n",
    "plt.axhline(y=0.0,ls=':',color='black')\n",
    "plt.xlim(0.0,1.0)\n",
    "plt.xlabel('redshift')\n",
    "plt.ylabel('magnitude normalised to (0,0)')\n",
    "plt.legend(frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5cfca1",
   "metadata": {},
   "source": [
    "### 3.4 Normalise the theory\n",
    "You can see that there is a vertical offset between the different theory curves.  \n",
    "We marginalise over this vertical offset, because we do not have enough information to constrain the vertical offset independently.\n",
    "In other words we use the data to determine what the vertical offset should be, but we do not care about what the answer is!\n",
    "(We are interested in the other parameters in the fit, but not that one.  The other parameters determine the *shape* of the curve.)\n",
    "\n",
    "The easiest way to normalise the curves is to take a weighted average of the difference between the data points and the theory in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa90c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mscript for each of these, which is the thing that determines the vertical normalisation \n",
    "mscr_om10_ox00 = np.sum((mu_om10_ox00-mu)/muerr**2)/np.sum(1./muerr**2)\n",
    "mscr_om03_ox00 = np.sum((mu_om03_ox00-mu)/muerr**2)/np.sum(1./muerr**2)\n",
    "mscr_om03_ox07 = np.sum((mu_om03_ox07-mu)/muerr**2)/np.sum(1./muerr**2)\n",
    "\n",
    "print(mscr_om10_ox00,mscr_om03_ox00,mscr_om03_ox07)\n",
    "\n",
    "mu_om10_ox00=mu_om10_ox00-mscr_om10_ox00\n",
    "mu_om03_ox00=mu_om03_ox00-mscr_om03_ox00\n",
    "mu_om03_ox07=mu_om03_ox07-mscr_om03_ox07\n",
    "\n",
    "# Repeat the plot and see how it changes\n",
    "# Plot it to see what it looks like, this is called a Hubble diagram\n",
    "plt.errorbar(zs,mu,yerr=muerr,fmt='.',elinewidth=0.7,markersize=4,alpha=0.5)\n",
    "plt.plot(zs,mu_om10_ox00,'-.',color='red',label='(1.0, 0.0)')\n",
    "plt.plot(zs,mu_om03_ox00,'--',color='blue',label='(0.3, 0.0)')\n",
    "plt.plot(zs,mu_om03_ox07,'-',color='green',label='(0.3, 0.7)')\n",
    "plt.xlim(0,1.0)\n",
    "plt.xlabel('redshift')\n",
    "plt.ylabel('magnitude')\n",
    "plt.legend(frameon=False)\n",
    "plt.show()\n",
    "\n",
    "# Now plot a Hubble diagram relative to the empty model (i.e. subtract the empty model from all the data and models)\n",
    "plt.errorbar(zs,mu-mu_om00_ox00,yerr=muerr,fmt='.',elinewidth=0.7,markersize=4,alpha=0.5)\n",
    "plt.plot(zs,mu_om10_ox00-mu_om00_ox00,'-.',color='red',label='(1.0, 0.0)')\n",
    "plt.plot(zs,mu_om03_ox00-mu_om00_ox00,'--',color='blue',label='(0.3, 0.0)')\n",
    "plt.plot(zs,mu_om03_ox07-mu_om00_ox00,'-',color='green',label='(0.3, 0.7)')\n",
    "plt.axhline(y=0.0,ls=':',color='black')\n",
    "plt.xlim(0,1.0)\n",
    "plt.xlabel('redshift')\n",
    "plt.ylabel('magnitude normalised to (0,0)')\n",
    "plt.legend(frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05856b83",
   "metadata": {},
   "source": [
    "## 3.5 Perform a fit to many models \n",
    "To find the best model that best matches the data use a $\\chi^2$ test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c082fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the arrays for the models you want to test, e.g. a range of Omega_m and Omega_Lambda models:\n",
    "n = 15                      # Increase this for a finer grid\n",
    "oms = np.linspace(0.0, 0.5, n)   # Array of matter densities\n",
    "ols = np.linspace(0.0, 1.0, n)   # Array of cosmological constant values\n",
    "chi2 = np.ones((n, n)) * np.inf  # Array to hold our chi2 values, set initially to super large values\n",
    "\n",
    "# Calculate Chi2 for each model\n",
    "for i, om in enumerate(oms):                                          # loop through matter densities\n",
    "       for j, ol in enumerate(ols):                                  # loop through cosmological constant densities\n",
    "            mu_model = dist_mod(zs, om=om, ol=ol)                     # calculate the distance modulus vs redshift for that model \n",
    "            mscr = np.sum((mu_model-mu)/muerr**2)/np.sum(1./muerr**2) # Calculate the vertical offset to apply\n",
    "            mu_model_norm = mu_model-mscr                             # Apply the vertical offset\n",
    "            chi2[i,j] = np.sum((mu_model_norm - mu) ** 2 / muerr**2)  # Calculate the chi2 and save it in a matrix \n",
    "            \n",
    "# Convert that to a likelihood and calculate the reduced chi2\n",
    "likelihood = np.exp(-0.5 * (chi2-np.amin(chi2)))  # convert the chi^2 to a likelihood (np.amin(chi2) calculates the minimum of the chi^2 array)\n",
    "chi2_reduced = chi2 / (len(mu)-2)                 # calculate the reduced chi^2, i.e. chi^2 per degree of freedom, where dof = number of data points minus number of parameters being fitted \n",
    "\n",
    "# Calculate the best fit values (where chi2 is minimum)\n",
    "indbest = np.argmin(chi2)                 # Gives index of best fit but where the indices are just a single number\n",
    "ibest   = np.unravel_index(indbest,[n,n]) # Converts the best fit index to the 2d version (i,j)\n",
    "print( 'Best fit values are (om,ol)=(%.3f,%.3f)'%( oms[ibest[0]], ols[ibest[1]] ) )\n",
    "print( 'Reduced chi^2 for the best fit is %0.2f'%chi2_reduced[ibest[0],ibest[1]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d24a490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot contours of 1, 2, and 3 sigma\n",
    "plt.contour(oms,ols,np.transpose(chi2-np.amin(chi2)),cmap=\"winter\",**{'levels':[2.30,6.18,11.83]})\n",
    "plt.plot(oms[ibest[0]], ols[ibest[1]],'x',color='black',label='(om,ol)=(%.3f,%.3f)'%( oms[ibest[0]], ols[ibest[1]]) )\n",
    "plt.xlabel(\"$\\Omega_m$\", fontsize=12)\n",
    "plt.ylabel(\"$\\Omega_\\Lambda$\", fontsize=12)\n",
    "plt.plot([oms[0],oms[1]], [ols[0],ols[1]],'-',color='black',label='Step size indicator' ) # Delete this line after making step size smaller!\n",
    "plt.legend(frameon=False)\n",
    "#plt.savefig('plots/contours.png', bbox_inches=\"tight\", transparent=True)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c161aa4e",
   "metadata": {},
   "source": [
    "INCLUDE PRIOR of OM=0.27 +/- 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a618bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the arrays for the models you want to test, e.g. a range of Omega_m and Omega_Lambda models:\n",
    "p_om = 0.3                 #  Prior om\n",
    "p_om_err = 0.05              # Prior om uncertainty\n",
    "n = 20                      # Increase this for a finer grid\n",
    "oms = np.linspace(0.0, 0.5, n)   # Array of matter densities\n",
    "ols = np.linspace(0, 1.0, n)   # Array of cosmological constant values\n",
    "chi2 = np.ones((n, n)) * np.inf  # Array to hold our chi2 values, set initially to super large values\n",
    "pri2 = np.ones((n, n)) * np.inf  # Array to hold our chi2 values, set initially to super large values\n",
    "tot2 = np.ones((n, n)) * np.inf \n",
    "\n",
    "# Calculate Chi2 for each model\n",
    "for i, om in enumerate(oms):                                          # loop through matter densities\n",
    "        for j, ol in enumerate(ols):                                  # loop through cosmological constant densities\n",
    "            mu_model = dist_mod(zs, om=om, ol=ol)                     # calculate the distance modulus vs redshift for that model \n",
    "            mscr = np.sum((mu_model-mu)/muerr**2)/np.sum(1./muerr**2) # Calculate the vertical offset to apply\n",
    "            mu_model_norm = mu_model-mscr                             # Apply the vertical offset\n",
    "            chi2[i,j] = np.sum((mu_model_norm - mu) ** 2 / muerr**2)  # Calculate the chi2 and save it in a matrix\n",
    "            pri2[i,j] = ((om - p_om) ** 2 / p_om_err**2)  # chi prior\n",
    "            tot2[i,j] = pri2[i,j] + chi2[i,j]                                   # Total chi^2\n",
    "            \n",
    "# Convert that to a likelihood and calculate the reduced chi2\n",
    "likelihood_t = np.exp(-0.5 * (tot2-np.amin(tot2)))  # convert the chi^2 to a likelihood (np.amin(chi2) calculates the minimum of the chi^2 array)\n",
    "tot2_reduced = tot2 / (len(mu)-2)                 # calculate the reduced chi^2, i.e. chi^2 per degree of freedom, where dof = number of data points minus number of parameters being fitted \n",
    "\n",
    "# Calculate the best fit values (where chi2 is minimum)\n",
    "indbest = np.argmin(tot2)                 # Gives index of best fit but where the indices are just a single number\n",
    "ibest   = np.unravel_index(indbest,[n,n]) # Converts the best fit index to the 2d version (i,j)\n",
    "print( 'Best fit values are (om,ol)=(%.3f,%.3f)'%( oms[ibest[0]], ols[ibest[1]] ) )\n",
    "print( 'Reduced tot^2 for the best fit is %0.2f'%tot2_reduced[ibest[0],ibest[1]] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6e27bc",
   "metadata": {},
   "source": [
    "Assume prior of flatness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a15965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the arrays for the models you want to test, e.g. a range of Omega_m and Omega_Lambda models:\n",
    "p_om = 0.3                 #  Prior om\n",
    "p_om_err = 0.05              # Prior om uncertainty\n",
    "p_omol = 1.00                 #  Prior om+0l\n",
    "p_omol_err = 0.05              # Prior om uncertainty\n",
    "n = 20                      # Increase this for a finer grid\n",
    "oms = np.linspace(0.0, 0.5, n)   # Array of matter densities\n",
    "ols = np.linspace(0, 1.0, n)   # Array of cosmological constant values\n",
    "chi2 = np.ones((n, n)) * np.inf  # Array to hold our chi2 values, set initially to super large values\n",
    "prif2 = np.ones((n, n)) * np.inf  # Array to hold our chi2 values, set initially to super large values\n",
    "tot2 = np.ones((n, n)) * np.inf \n",
    "prim2 = np.ones((n, n)) * np.inf \n",
    "\n",
    "# Calculate Chi2 for each model\n",
    "for i, om in enumerate(oms):                                          # loop through matter densities\n",
    "        for j, ol in enumerate(ols):                                  # loop through cosmological constant densities\n",
    "            mu_model = dist_mod(zs, om=om, ol=ol)                     # calculate the distance modulus vs redshift for that model \n",
    "            mscr = np.sum((mu_model-mu)/muerr**2)/np.sum(1./muerr**2) # Calculate the vertical offset to apply\n",
    "            mu_model_norm = mu_model-mscr                             # Apply the vertical offset\n",
    "            chi2[i,j] = np.sum((mu_model_norm - mu) ** 2 / muerr**2)  # Calculate the chi2 and save it in a matrix\n",
    "            prif2[i,j] = (((om+ol) - p_omol) ** 2 / p_omol_err**2)  # chi prior\n",
    "            prim2[i,j] = ((om - p_om) ** 2 / p_om_err**2)  # chi prior\n",
    "            tot2[i,j] = prif2[i,j] + chi2[i,j] + prim2[i,j]                                  # Total chi^2\n",
    "            \n",
    "# Convert that to a likelihood and calculate the reduced chi2\n",
    "likelihood_t = np.exp(-0.5 * (tot2-np.amin(tot2)))  # convert the chi^2 to a likelihood (np.amin(chi2) calculates the minimum of the chi^2 array)\n",
    "tot2_reduced = tot2 / (len(mu)-2)                 # calculate the reduced chi^2, i.e. chi^2 per degree of freedom, where dof = number of data points minus number of parameters being fitted \n",
    "\n",
    "# Calculate the best fit values (where chi2 is minimum)\n",
    "indbest = np.argmin(tot2)                 # Gives index of best fit but where the indices are just a single number\n",
    "ibest   = np.unravel_index(indbest,[n,n]) # Converts the best fit index to the 2d version (i,j)\n",
    "print( 'Best fit values are (om,ol)=(%.3f,%.3f)'%( oms[ibest[0]], ols[ibest[1]] ) )\n",
    "print( 'Reduced tot^2 for the best fit is %0.4f'%tot2_reduced[ibest[0],ibest[1]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765b2dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(mu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935f90ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot contours of 1, 2, and 3 sigma\n",
    "plt.contour(oms,ols,np.transpose(tot2-np.amin(tot2)),cmap=\"winter\",**{'levels':[2.30,6.18,11.83]})\n",
    "#plt.contour(oms,ols,np.transpose(prif2-np.amin(prif2)),cmap=\"winter\",**{'levels':[2.30,6.18,11.83]})\n",
    "plt.contourf(oms,ols,np.transpose(prif2-np.amin(prif2)),**{'levels=[2.30,6.18,11.83]})\n",
    "plt.contour(oms,ols,np.transpose(chi2-np.amin(chi2)),cmap=\"winter\",**{'levels':[2.30,6.18,11.83]})\n",
    "plt.contour(oms,ols,np.transpose(prim2-np.amin(prim2)),cmap=\"winter\",**{'levels':[2.30,6.18,11.83]})\n",
    "plt.plot(oms[ibest[0]], ols[ibest[1]],'x',color='black',label='(om,ol)=(%.3f,%.3f)'%( oms[ibest[0]], ols[ibest[1]]) )\n",
    "plt.xlabel(\"$\\Omega_m$\", fontsize=12)\n",
    "plt.ylabel(\"$\\Omega_\\Lambda$\", fontsize=12)\n",
    "plt.plot([oms[0],oms[1]], [ols[0],ols[1]],'-',color='black',label='Step size indicator' ) # Delete this line after making step size smaller!\n",
    "plt.legend(frameon=False)\n",
    "#plt.savefig('plots/contours.png', bbox_inches=\"tight\", transparent=True)\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61a15e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
